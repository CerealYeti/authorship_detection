# -*- coding: utf-8 -*-
"""AuthorshipPart2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11f7mzzjdtkAp3CyW0l1N3NrNbm5IPtw5
"""

# -*- coding: utf-8 -*-
"""AuthorshipPart2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11f7mzzjdtkAp3CyW0l1N3NrNbm5IPtw5
"""

# -*- coding: utf-8 -*-
"""
Created on Mon Nov 12 19:21:58 2018
This is a skeleton with the function headers for
the functions you should implement.
You can add more functions

@author: weissr, geddy, joseph, dylana
"""

import nltk
from nltk.corpus import gutenberg
from nltk.probability import FreqDist
import csv



'''
-------------------------
Features
-------------------------
'''
def avg_word_len(raw, words):
    return len(raw) / len(words)
    
def lexical_div(words):
    '''
    compute the lexical diversity for a list of words
    '''  
    n_Owords = len(words)
    n_Ovocab = len(set(words))
    l_div = n_Ovocab / n_Owords 
    
    return l_div 

def n_hapax(words):
    fdist = FreqDist(words)
    return float(len(fdist.hapaxes()))

def hapax_ratio(words):
    fdist = FreqDist(words)
    hap_n = len(fdist.hapaxes())     
    return hap_n / len(words)


def avg_sent_len(words, sents):
    words = len(words)
    sents = len(sents)
    s_len = words / sents    
    return s_len 

def avg_sent_complexity(words, sents):
    '''Return the average number of phrases per sentence'''
    n_sents = len(sents)
    n_phrases = 0
    for s in sents:
        n_phrases += 1 # Every sentence has at least 1 phrase
        for token in s:
            if token in ":;,": # Tally phrase separators
                n_phrases += 1
    return n_phrases / n_sents


'''
--------------------------
Compare Signatures
--------------------------
'''

def compute_signature(raw, words, sentences, author):
    sig = [author]
    sig.append(avg_word_len(raw, words))
    sig.append(lexical_div(words))
    sig.append(hapax_ratio(words))
    sig.append(avg_sent_len(words, sentences))
    sig.append(avg_sent_complexity(words, sentences))
    return sig
    
    
def compare_signatures(sig1, sig2, weights):
    '''Return a non-negative real number indicating the similarity of two 
    linguistic signatures. The smaller the number the more similar the 
    signatures. Zero indicates identical signatures.
    sig1 and sig2 are 6 element lists with the following elements
    0  : author name (a string)
    1  : average word length (float)
    2  : lexical diversity (float)
    3  : Hapax Legomana Ratio (float)
    4  : average sentence length (float)
    5  : average sentence complexity (float)
    weight is a list of multiplicative weights to apply to each
    linguistic feature. weight[0] is ignored.
    '''
    score = 0.0
    for i in range(1, len(sig1)):
        score += abs(float(sig1[i]) - float(sig2[i])) * weights[i -1]
    return  score


'''
--------------------------
Reading and Writing files
--------------------------
'''

def write_signatures(sig_list, o_filename):
    with open(o_filename, 'w') as writeFile:
        writer = csv.writer(writeFile)
        writer.writerow(headings)
        for sig in sig_list:
            row = []
            for i in range(len(sig)):
                row.append(sig[i])
            writer.writerow(row)
    writeFile.close()
   
def read_signatures(in_filename):
    data = []
    with open(in_filename, 'r') as readFile:
        reader = csv.reader(readFile, delimiter = ',')
        line_count = 0
        for row in reader:
            if not line_count == 0:
                if row:
                    data.append(row)
            line_count += 1
    readFile.close()
    return data
    
    
def read_text(in_filename):
    try:
        path = 'corpora/gutenberg/' + in_filename
        file = nltk.data.find(path)
        f_in = open(file, encoding='utf-8')
        raw = f_in.read()
        f_in.close()        
    except:
        print('failed to open', in_filename)
        return '' 
    return raw


'''
--------------------------
Printing the output
--------------------------
'''

def print_sig_table(sig_list):    
    for head in headings:
        print('{:<25}'.format(head), end='')
    print('\n', "-" * 125)
    for items in sig_list:
        #print('\n')
        for item in items:
            print('{:<25}'.format(item), end='')
    print('\n\n')
def print_scores(sig_list, m_sig_list):
    #print('table of scores')
    
    for head in headings:
        print('{:<25}'.format(head), end='')
    print('\n', '-' * 125)
    for items in sig_list:
        print('\n', end='')
        for item in items:
            print('{:<25}'.format(item), end='')
    for items in m_sig_list:
        print('\n', end='')
        for item in items:
            print('{:<25}'.format(item), end='')
            
def matches_table(scores):
    #print("\n\nComparison Table")
    print('\n')
    print("{:<25}".format('title'), end='')
    for i in range(4):
        if i == 0:
            print("{:<25}".format(scores[i][0]), end='')
        else:
            print("{:<25}".format(scores[i + (18 * i)][0]), end='')
    print('\n', "-" * 125, '\n')
    for i in range(18):
        print("{:<25}{:<25}{:<25}{:<25}{:<25}".format(scores[i][1], scores[i][2], scores[i+18][2], scores[i+(18*2)][2], scores[i+(18*3)][2]))

    #print("\n%s matches %s with a score of %f" % (score[0], score[1], score[2]))

def best_matches(scores):
    matches = []
    name = scores[0][0]
    best_score = 1000
    best_name = ''
    for score in scores:
        if score[0] == name:
            if score[2] < best_score:
                best_score = score[2]
                best_name = score[1]
        else:
            matches.append((name, best_name, best_score))
            name = score[0]
            best_score = score[2]
            best_name = score[1]
    matches.append((name, best_name, best_score))
    
    print("\nBest Matches")
    print("-" * 125)
    for match in matches:
        print('%s best match is %s with a score of %f' % (match[0], match[1], match[2]))
    print('\n\n')
'''
-------------------------
main
-------------------------
'''
if __name__ == '__main__':
    weights = [10, 20, 50, 0.04, 10]
    sig_list = []
    global headings 
    headings = ['Text', 'Average Word Length', 'Lexical Diversity', 'Hapax Legomana Ratio','Average Sent Length', 'Average Sentence Complexity']
    try:
        sig_list = read_signatures('test.csv')
    except:     
        fileids = gutenberg.fileids()
        for fid in fileids: 
            # compute features, make a list of features
            if not ('mystery' in fid):
                raw = gutenberg.raw(fid)
                words = gutenberg.words(fid)
                sents = gutenberg.sents(fid)
                signature = compute_signature(raw, words, sents, fid)
                sig_list.append(signature)
        write_signatures(sig_list, 'test.csv')

    mystery_files = ['mystery2a.txt', 'mystery6a.txt', 'mystery7a.txt', 'mystery8a.txt']
    m_sig_list = []
    for file in mystery_files:
        raw_text = read_text(file)    
        '''
        put mystery files in corpora/gutenberg
        '''
        m_raw = gutenberg.raw(file)
        m_words = gutenberg.words(file)
        m_sents = gutenberg.sents(file)
        m_sig = compute_signature(m_raw, m_words, m_sents, file)
        m_sig_list.append(m_sig)
    
    print_scores(sig_list, m_sig_list)
    scores = []
    for m_signature in m_sig_list:
        for sig in sig_list:
            scores.append((m_signature[0], sig[0], compare_signatures(sig, m_signature, weights)))
    
    matches_table(scores)
    best_matches(scores)

    #0, 13. 20, 30, 0.05, 8

nltk.download('gutenberg')
nltk.download('genesis')
nltk.download('inaugural')
nltk.download('nps_chat')
nltk.download('webtext')
nltk.download('treebank')
nltk.download('punkt')

"""sig = ['Jane Austen - Emma']
    sig.append(avg_word_len(gutenberg.words(emma)))
    sig.append(lexical_div(gutenberg.words(emma)))
    sig.append(hapax_ratio(gutenberg.words(emma)))
    sig.append(avg_sent_len(gutenberg.words(emma), gutenberg.sents(emma)))
    sig.append(avg_sent_complexity(gutenberg.words(emma), gutenberg.sents(emma)))
"""